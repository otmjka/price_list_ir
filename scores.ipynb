{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let s_i - boolean score denoting a match (or absence thereof) between q and the ith zone\n",
    "\n",
    "**!!!** For instance, the Boolean score from a zone could be 1 if all the query term(s) occur in that zone, and zero otherwise; indeed, it could be any Boo- lean function that maps the presence of query terms in a zone to 0, 1. Then, the weighted zone score is defined to be\n",
    "\n",
    "можно какие то условия настроить, чтобы лишнее не попадало\n",
    "\n",
    "```\n",
    "sum(i=1, l)(g_i, s_i)\n",
    "```\n",
    "\n",
    "ranked Boolean retrieval\n",
    "\n",
    "```\n",
    "query shakespeare\n",
    "zones: author, title and body\n",
    "s = The Boolean score function for a zone takes on the value 1 if the query term shakespeare is present in the zone, and zero otherwise.\n",
    "g1 = 0.2\n",
    "g2 = 0.3\n",
    "g3 = 0.5\n",
    "\n",
    "title(g2) and body(g3) = 0.8\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we now show how we may compute weighted zone scores directly from inverted indexes. \n",
    "\n",
    "если tn составное то s \"если все части tn встретились\"\n",
    "\n",
    "при прохождении term: docs не просто добавляем документ во множество результатов мы вычисляем релевантность каждого документа\n",
    "\n",
    "все таки это делается после индексирования\n",
    "\n",
    "term0 [doc777, ...] term1 [doc0, ..., doc777]\n",
    "p1 = postings(term0) => doc777\n",
    "p2 = postings(term1) => doc0\n",
    "\n",
    "p2 <- next(p2)\n",
    "\n",
    "p1 = postings(term0) => doc777\n",
    "p2 = postings(term1) => ...\n",
    "\n",
    "p2 <- next(p2)\n",
    "\n",
    "p1 = postings(term0) => doc777\n",
    "p2 = postings(term1) => doc777\n",
    "\n",
    "docID(p1) == docID(p2)\n",
    "docID(p1) # doc777\n",
    "\n",
    "scores[docID(p1)] = weighted_zone(p1, p2, g)\n",
    "\n",
    "## ??? weighted_zone ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.1.2 Learning weights\n",
    "\n",
    "training examples\n",
    "q\n",
    "d\n",
    "score(q, d)\n",
    "g_i учится на **данных** примерах так, чтобы полученные оценки **аппроксимировали** оценки релевантности **тренировочных** примеров.\n",
    "\n",
    "наверное, валидационные данные, тренировочные и тестовые\n",
    "\n",
    "как подбор коэфицентов линейной функции от булевых признаков вхождения в соответствующие зоны\n",
    "as learning a linear function of the Boolean match scores contributed by the various zones.\n",
    "как обучение линейной функции "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "term frequency and is denoted `tf_(t,d)`\n",
    "`document frequency df_t` - defined to be the number of documents in the collection that con- tain a term t.\n",
    "\n",
    "This is because in trying to discriminate between documents for the purpose of scoring it is better to use a document-level statistic (such as the number of documents containing a term) than to use a collection-wide statistic for the term\n",
    "\n",
    "{term: docs} df - defined to be the number of documents in the collection that con- tain a term t.\n",
    "\n",
    "N - docs count\n",
    "`inverse document frequency (idf)` of a term t as follows:\n",
    "`idf_t = log(N/df_t)` *base 10*.\n",
    "\n",
    "Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2.2 Tf-idf weighting\n",
    "\n",
    "term frequency and inverse document frequency\n",
    "The tf-idf weighting scheme assigns to term `t` a weight in document `d` given by\n",
    "tf-idft,d = tft,d × idft.\n",
    "\n",
    "\n",
    "## vector\n",
    "\n",
    "At this point, we may view each document as a vector with one component\n",
    "corresponding to each term in the dictionary, together with a weight for each component that is given by (6.8). For dictionary terms that do not occur in a document, this weight is zero. This vector form will prove to be crucial to scoring and ranking; we will develop these ideas in Section 6.3. As a first step, we introduce the overlap score measure: the score of a document d is the sum, over all query terms, of the number of times each of the query terms occurs in d. We can refine this idea so that we add up not the number of occurrences of each query term t in d, but instead the tf-idf weight of each term in d.\n",
    "Score(q, d) = ∑ tf-idft,d.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.3 The vector space model for scoring\n",
    "\n",
    " vector space model \n",
    " \n",
    "## Dot products\n",
    "\n",
    "V⃗ (d) vector derived from document d,\n",
    "with one component in the vector for each dictionary term.\n",
    "\n",
    "в котором каждому термину соответствует отдельная ось\n",
    "\n",
    "сходство двух документов в векторном пространстве.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
